{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c231994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2941693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\pinkpigma\\pinkpigma的同步盘\\KDD研二上\\可解释性框架-工作\\数据\\patient_diagnoses2000_ndc300_delete1.csv',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a8f3c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       heartrate_min  heartrate_max  heartrate_mean  sysbp_min  sysbp_max  \\\n",
      "0               89.0          142.0      107.085714       95.0      165.0   \n",
      "1               60.0           97.0       81.588235       77.0      153.0   \n",
      "2               51.0           79.0       65.333333       98.0      133.0   \n",
      "3               29.0           52.0       34.844828       93.0      165.0   \n",
      "4               78.0          118.0       93.260870      125.0      178.0   \n",
      "...              ...            ...             ...        ...        ...   \n",
      "38319           58.0           82.0       65.473684      129.0      188.0   \n",
      "38320           81.0          113.0       94.000000      100.0      134.0   \n",
      "38321           67.0          107.0       85.250000       61.0      157.0   \n",
      "38322           65.0           86.0       71.120000       92.0      130.0   \n",
      "38323           66.0           96.0       81.185185      143.0      187.0   \n",
      "\n",
      "       sysbp_mean  diasbp_min  diasbp_max  diasbp_mean  meanbp_min  ...  0543  \\\n",
      "0      124.850000        43.0        87.0    66.625000   59.000000  ...     0   \n",
      "1      119.000000        46.0        86.0    65.944444   56.000000  ...     0   \n",
      "2      109.151515        48.0        81.0    61.090909   71.000000  ...     0   \n",
      "3      121.954545        29.0       102.0    46.250000   45.000000  ...     0   \n",
      "4      147.428571        75.0       117.0    92.571429   53.000000  ...     0   \n",
      "...           ...         ...         ...          ...         ...  ...   ...   \n",
      "38319  158.500000        35.0        80.0    54.111111   60.000000  ...     0   \n",
      "38320  119.869565        52.0        92.0    71.739130   63.000000  ...     0   \n",
      "38321  118.964286        31.0        67.0    52.142857   40.000000  ...     0   \n",
      "38322  109.000000        45.0        72.0    56.100000   62.000000  ...     0   \n",
      "38323  161.692308        52.0        82.0    64.576923   82.333298  ...     0   \n",
      "\n",
      "       2181  2550  86813  99631  E8500  1129  1504  76409  7703  \n",
      "0         0     0      0      0      0     0     0      0     0  \n",
      "1         0     0      0      0      0     0     0      0     0  \n",
      "2         0     0      0      0      0     0     0      0     0  \n",
      "3         0     0      0      0      0     0     0      0     0  \n",
      "4         0     0      0      0      0     0     0      0     0  \n",
      "...     ...   ...    ...    ...    ...   ...   ...    ...   ...  \n",
      "38319     0     0      0      0      0     0     0      0     0  \n",
      "38320     0     0      0      0      0     0     0      0     0  \n",
      "38321     0     0      0      0      0     0     0      0     0  \n",
      "38322     0     0      0      0      0     0     0      0     0  \n",
      "38323     0     0      0      0      0     0     0      0     0  \n",
      "\n",
      "[38324 rows x 2071 columns]\n"
     ]
    }
   ],
   "source": [
    "#输入少了一个体重\n",
    "x = df.iloc[:,3:3+71+2000]\n",
    "y = df.iloc[:,3+71+2000:]\n",
    "# print(x)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn. preprocessing import LabelEncoder \n",
    "x['gender'] = LabelEncoder().fit_transform(x['gender'])\n",
    "x['ethnicity'] = LabelEncoder().fit_transform(x['ethnicity'])\n",
    "x['ethnicity_grouped'] = LabelEncoder().fit_transform(x['ethnicity_grouped'])\n",
    "x['first_hosp_stay'] = LabelEncoder().fit_transform(x['first_hosp_stay'])\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "x = imp.fit_transform(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51af8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "from keras import backend as K\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b1b0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "def Jaccard_Loss(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "\n",
    "    y_true_expand = K.expand_dims(y_true, axis=0)\n",
    "    y_pred_expand = K.expand_dims(y_pred, axis=-1)\n",
    "\n",
    "    fenzi = K.dot(y_true_expand, y_pred_expand)\n",
    "\n",
    "    fenmu_1 = K.sum(y_true, keepdims=True)\n",
    "\n",
    "    fenmu_2 = K.ones_like(y_true_expand) - y_true_expand\n",
    "    fenmu_2 = K.dot(fenmu_2, y_pred_expand)\n",
    "\n",
    "    return K.mean((tf.constant([[1]], dtype=tf.float32) - (fenzi / (fenmu_1 + fenmu_2))), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def JI(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "#     K.print_tensor(y_true, message='y_true = ')\n",
    "\n",
    "    threshold_value = 0.3\n",
    "\n",
    "    y_pred = K.cast(K.greater(y_pred, threshold_value), K.floatx())\n",
    "#     K.print_tensor(y_pred, message='y_pred = ')\n",
    "    fenzi = K.sum(y_true * y_pred, keepdims=True)\n",
    "    # true_positives_sum = K.sum(true_positives, keepdims=True)\n",
    "    fenmu = K.sum(K.cast((K.greater(y_true + y_pred, 0.8)), K.floatx()), keepdims=True)\n",
    "\n",
    "    return K.mean(fenzi / fenmu, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def deep_model(feature_dim,label_dim, layer_num=1):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    print(\"create model. feature_dim ={}, label_dim ={}\".format(feature_dim, label_dim))\n",
    "    model.add(Dense(512, activation='relu', input_dim=feature_dim))\n",
    "    \n",
    "    for i in range(layer_num):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(label_dim, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[JI])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51fef1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "callbacks_list = [\n",
    "    # This callback will interrupt training when we have stopped improving\n",
    "     keras.callbacks.EarlyStopping(\n",
    "    # This callback will monitor the validation accuracy of the model\n",
    "    monitor='val_loss',\n",
    "     # Training will be interrupted when the accuracy\n",
    "     # has stopped improving for *more* than 1 epochs (i.e. 2 epochs)\n",
    "     patience=20,\n",
    "    ),\n",
    "#     keras.callbacks.ReduceLROnPlateau(\n",
    "#      # This callback will monitor the validation loss of the model\n",
    "#      monitor='val_loss',\n",
    "#      # It will divide the learning by 10 when it gets triggered\n",
    "#      factor=0.1,\n",
    "#      # It will get triggered after the validation loss has stopped improving\n",
    "#      # for at least 10 epochs\n",
    "#      patience=10,\n",
    "#     ),\n",
    "    # This callback will save the current weights after every epoch\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "     filepath=r'C:\\Users\\pinkpigma\\pinkpigma的同步盘\\KDD研二上\\jupyter\\model\\my_model.h5', # Path to the destination model file\n",
    "     # The two arguments below mean that we will not overwrite the\n",
    "    # model file unless `val_loss` has improved, which\n",
    "    # allows us to keep the best model every seen during training.\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0f03a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def train_deep(X_train,y_train,X_test,y_test, layer_num=1, callbacks_list = []):\n",
    "    feature_dim = X_train.shape[1]\n",
    "    label_dim = y_train.shape[1]\n",
    "    model = deep_model(feature_dim,label_dim, layer_num)\n",
    "    model.summary()\n",
    "    history = model.fit(X_train,y_train,batch_size=256, epochs=1000,callbacks=callbacks_list,validation_data=(X_test,y_test), verbose=1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900ba664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,477,420\n",
      "Trainable params: 1,477,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4602 - JI: 0.0335 - val_loss: 0.1181 - val_JI: 0.0262\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1256 - JI: 0.0316 - val_loss: 0.1226 - val_JI: 0.0342\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1195 - JI: 0.0288 - val_loss: 0.1141 - val_JI: 0.0258\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1204 - JI: 0.0264 - val_loss: 0.1154 - val_JI: 0.0266\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1126 - JI: 0.0290 - val_loss: 0.1093 - val_JI: 0.0235\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1120 - JI: 0.0349 - val_loss: 0.1074 - val_JI: 0.0297\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1116 - JI: 0.0411 - val_loss: 0.1080 - val_JI: 0.0411\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1073 - JI: 0.0460 - val_loss: 0.1068 - val_JI: 0.0486\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1073 - JI: 0.0533 - val_loss: 0.1053 - val_JI: 0.0438\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1044 - JI: 0.0562 - val_loss: 0.1046 - val_JI: 0.0597\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1036 - JI: 0.0621 - val_loss: 0.1032 - val_JI: 0.0564\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1026 - JI: 0.0641 - val_loss: 0.1024 - val_JI: 0.0672\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1019 - JI: 0.0697 - val_loss: 0.1009 - val_JI: 0.0669\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1006 - JI: 0.0732 - val_loss: 0.1014 - val_JI: 0.0803\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1015 - JI: 0.0736 - val_loss: 0.1017 - val_JI: 0.0633\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1005 - JI: 0.0747 - val_loss: 0.1016 - val_JI: 0.0773\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0991 - JI: 0.0798 - val_loss: 0.0997 - val_JI: 0.0828\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0984 - JI: 0.0837 - val_loss: 0.1003 - val_JI: 0.0681\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0992 - JI: 0.0857 - val_loss: 0.1005 - val_JI: 0.0667\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0979 - JI: 0.0859 - val_loss: 0.0986 - val_JI: 0.0843\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0973 - JI: 0.0888 - val_loss: 0.0978 - val_JI: 0.0884\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0969 - JI: 0.0910 - val_loss: 0.1212 - val_JI: 0.0686\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1019 - JI: 0.0821 - val_loss: 0.0994 - val_JI: 0.0854\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0980 - JI: 0.0879 - val_loss: 0.0990 - val_JI: 0.0948\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0983 - JI: 0.0920 - val_loss: 0.0988 - val_JI: 0.1017\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0970 - JI: 0.0961 - val_loss: 0.0968 - val_JI: 0.0910\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0966 - JI: 0.0983 - val_loss: 0.0971 - val_JI: 0.1049\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0986 - JI: 0.0998 - val_loss: 0.0967 - val_JI: 0.0968\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0952 - JI: 0.1021 - val_loss: 0.0960 - val_JI: 0.0986\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0944 - JI: 0.1053 - val_loss: 0.0960 - val_JI: 0.0930\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0944 - JI: 0.1066 - val_loss: 0.0974 - val_JI: 0.0987\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.1073 - JI: 0.1077 - val_loss: 0.0962 - val_JI: 0.0974\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0940 - JI: 0.1097 - val_loss: 0.0956 - val_JI: 0.0988\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0962 - JI: 0.1108 - val_loss: 0.0954 - val_JI: 0.1096\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0935 - JI: 0.1132 - val_loss: 0.0960 - val_JI: 0.1010\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0964 - JI: 0.1121 - val_loss: 0.0950 - val_JI: 0.1168\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0924 - JI: 0.1164 - val_loss: 0.0948 - val_JI: 0.1065\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0925 - JI: 0.1163 - val_loss: 0.0946 - val_JI: 0.1083\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0920 - JI: 0.1191 - val_loss: 0.0953 - val_JI: 0.1206\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0915 - JI: 0.1220 - val_loss: 0.0950 - val_JI: 0.0996\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0913 - JI: 0.1233 - val_loss: 0.0955 - val_JI: 0.1071\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0911 - JI: 0.1246 - val_loss: 0.0942 - val_JI: 0.1190\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0912 - JI: 0.1248 - val_loss: 0.0939 - val_JI: 0.1171\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0909 - JI: 0.1269 - val_loss: 0.0939 - val_JI: 0.1145\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0904 - JI: 0.1284 - val_loss: 0.0940 - val_JI: 0.1245\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0903 - JI: 0.1312 - val_loss: 0.0942 - val_JI: 0.1165\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0903 - JI: 0.1318 - val_loss: 0.0942 - val_JI: 0.1305\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0901 - JI: 0.1322 - val_loss: 0.0943 - val_JI: 0.1175\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0905 - JI: 0.1331 - val_loss: 0.0937 - val_JI: 0.1289\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0895 - JI: 0.1372 - val_loss: 0.0937 - val_JI: 0.1241\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0895 - JI: 0.1377 - val_loss: 0.0938 - val_JI: 0.1342\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0892 - JI: 0.1390 - val_loss: 0.0934 - val_JI: 0.1399\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0897 - JI: 0.1370 - val_loss: 0.0936 - val_JI: 0.1206\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0896 - JI: 0.1372 - val_loss: 0.0936 - val_JI: 0.1292\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0891 - JI: 0.1408 - val_loss: 0.0939 - val_JI: 0.1124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0886 - JI: 0.1445 - val_loss: 0.0928 - val_JI: 0.1415\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0884 - JI: 0.1456 - val_loss: 0.0932 - val_JI: 0.1260\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0881 - JI: 0.1482 - val_loss: 0.0931 - val_JI: 0.1185\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0911 - JI: 0.1300 - val_loss: 0.0937 - val_JI: 0.1215\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0887 - JI: 0.1428 - val_loss: 0.0943 - val_JI: 0.1319\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0885 - JI: 0.1452 - val_loss: 0.0934 - val_JI: 0.1256\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0879 - JI: 0.1487 - val_loss: 0.0929 - val_JI: 0.1450\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0876 - JI: 0.1505 - val_loss: 0.0931 - val_JI: 0.1380\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0882 - JI: 0.1512 - val_loss: 0.0930 - val_JI: 0.1362\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0920 - JI: 0.1517 - val_loss: 0.0946 - val_JI: 0.1396\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0890 - JI: 0.1511 - val_loss: 0.0931 - val_JI: 0.1420\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0895 - JI: 0.1541 - val_loss: 0.0933 - val_JI: 0.1335\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0896 - JI: 0.1533 - val_loss: 0.0936 - val_JI: 0.1444\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0869 - JI: 0.1569 - val_loss: 0.0931 - val_JI: 0.1374\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0866 - JI: 0.1595 - val_loss: 0.0937 - val_JI: 0.1349\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0864 - JI: 0.1611 - val_loss: 0.0932 - val_JI: 0.1265\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0862 - JI: 0.1616 - val_loss: 0.0935 - val_JI: 0.1243\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0863 - JI: 0.1625 - val_loss: 0.0938 - val_JI: 0.1227\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0867 - JI: 0.1612 - val_loss: 0.0957 - val_JI: 0.1248\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0868 - JI: 0.1590 - val_loss: 0.0935 - val_JI: 0.1388\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0859 - JI: 0.1646 - val_loss: 0.0944 - val_JI: 0.1268\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0857 - JI: 0.1656 - val_loss: 0.0936 - val_JI: 0.1405\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0855 - JI: 0.1679 - val_loss: 0.0937 - val_JI: 0.1343\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0853 - JI: 0.1699 - val_loss: 0.0932 - val_JI: 0.1387\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0851 - JI: 0.1705 - val_loss: 0.0932 - val_JI: 0.1452\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0849 - JI: 0.1725 - val_loss: 0.0938 - val_JI: 0.1418\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0846 - JI: 0.1737 - val_loss: 0.0936 - val_JI: 0.1493\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0845 - JI: 0.1758 - val_loss: 0.0934 - val_JI: 0.1465\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0847 - JI: 0.1741 - val_loss: 0.0942 - val_JI: 0.1368\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0841 - JI: 0.1772 - val_loss: 0.0936 - val_JI: 0.1410\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0844 - JI: 0.1772 - val_loss: 0.0942 - val_JI: 0.1357\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0838 - JI: 0.1797 - val_loss: 0.0944 - val_JI: 0.1299\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0836 - JI: 0.1821 - val_loss: 0.0942 - val_JI: 0.1368\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0836 - JI: 0.1833 - val_loss: 0.0949 - val_JI: 0.1307\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0835 - JI: 0.1830 - val_loss: 0.0946 - val_JI: 0.1544\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0838 - JI: 0.1824 - val_loss: 0.0939 - val_JI: 0.1382\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0833 - JI: 0.1855 - val_loss: 0.0945 - val_JI: 0.1455\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0831 - JI: 0.1866 - val_loss: 0.0950 - val_JI: 0.1302\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0828 - JI: 0.1894 - val_loss: 0.0940 - val_JI: 0.1442\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0828 - JI: 0.1892 - val_loss: 0.0943 - val_JI: 0.1348\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0826 - JI: 0.1907 - val_loss: 0.0944 - val_JI: 0.1415\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0830 - JI: 0.1888 - val_loss: 0.0945 - val_JI: 0.1408\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0824 - JI: 0.1931 - val_loss: 0.0953 - val_JI: 0.1436\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0821 - JI: 0.1947 - val_loss: 0.0947 - val_JI: 0.1460\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0820 - JI: 0.1950 - val_loss: 0.0956 - val_JI: 0.1321\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0818 - JI: 0.1971 - val_loss: 0.0960 - val_JI: 0.1489\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0817 - JI: 0.1980 - val_loss: 0.0950 - val_JI: 0.1373\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0816 - JI: 0.1985 - val_loss: 0.0954 - val_JI: 0.1381\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0813 - JI: 0.2007 - val_loss: 0.0955 - val_JI: 0.1364\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0813 - JI: 0.2014 - val_loss: 0.0961 - val_JI: 0.1383\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.0814 - JI: 0.2013 - val_loss: 0.0964 - val_JI: 0.1476\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_deep(X_train,y_train,X_test,y_test,1, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68098bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,740,076\n",
      "Trainable params: 1,740,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 3s 18ms/step - loss: 0.2716 - JI: 0.0239 - val_loss: 0.1131 - val_JI: 0.0097\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1211 - JI: 0.0204 - val_loss: 0.1133 - val_JI: 0.0205\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1222 - JI: 0.0225 - val_loss: 0.1099 - val_JI: 0.0375\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1166 - JI: 0.0337 - val_loss: 0.1112 - val_JI: 0.0324\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1103 - JI: 0.0385 - val_loss: 0.1099 - val_JI: 0.0421\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1079 - JI: 0.0462 - val_loss: 0.1057 - val_JI: 0.0517\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1231 - JI: 0.0524 - val_loss: 0.1055 - val_JI: 0.0593\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1066 - JI: 0.0574 - val_loss: 0.1047 - val_JI: 0.0580\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.1053 - JI: 0.0643 - val_loss: 0.1049 - val_JI: 0.0572\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1041 - JI: 0.0679 - val_loss: 0.1017 - val_JI: 0.0830\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1015 - JI: 0.0755 - val_loss: 0.1004 - val_JI: 0.0685\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1028 - JI: 0.0746 - val_loss: 0.1007 - val_JI: 0.0694\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1018 - JI: 0.0804 - val_loss: 0.1001 - val_JI: 0.0777\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0990 - JI: 0.0837 - val_loss: 0.0984 - val_JI: 0.0947\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0973 - JI: 0.0894 - val_loss: 0.0978 - val_JI: 0.0876\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0970 - JI: 0.0919 - val_loss: 0.0970 - val_JI: 0.0960\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0964 - JI: 0.0953 - val_loss: 0.0982 - val_JI: 0.0955\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0962 - JI: 0.0963 - val_loss: 0.0973 - val_JI: 0.0944\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0954 - JI: 0.0988 - val_loss: 0.0961 - val_JI: 0.0928\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0950 - JI: 0.1019 - val_loss: 0.0959 - val_JI: 0.1010\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0943 - JI: 0.1039 - val_loss: 0.0960 - val_JI: 0.0940\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0944 - JI: 0.1048 - val_loss: 0.0952 - val_JI: 0.1136\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0935 - JI: 0.1097 - val_loss: 0.0960 - val_JI: 0.1159\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0939 - JI: 0.1087 - val_loss: 0.0961 - val_JI: 0.1102\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0934 - JI: 0.1113 - val_loss: 0.0954 - val_JI: 0.1179\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0927 - JI: 0.1134 - val_loss: 0.0953 - val_JI: 0.1012\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0931 - JI: 0.1132 - val_loss: 0.0941 - val_JI: 0.1024\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0924 - JI: 0.1167 - val_loss: 0.0966 - val_JI: 0.1006\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0922 - JI: 0.1175 - val_loss: 0.0938 - val_JI: 0.1095\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0919 - JI: 0.1207 - val_loss: 0.0952 - val_JI: 0.1305\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0917 - JI: 0.1218 - val_loss: 0.0949 - val_JI: 0.1151\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0909 - JI: 0.1257 - val_loss: 0.0936 - val_JI: 0.1146\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0917 - JI: 0.1235 - val_loss: 0.0963 - val_JI: 0.1317\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0911 - JI: 0.1243 - val_loss: 0.0938 - val_JI: 0.1291\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0904 - JI: 0.1297 - val_loss: 0.0941 - val_JI: 0.1257\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0900 - JI: 0.1317 - val_loss: 0.0944 - val_JI: 0.1072\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0901 - JI: 0.1328 - val_loss: 0.0963 - val_JI: 0.1046\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0897 - JI: 0.1341 - val_loss: 0.0934 - val_JI: 0.1262\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0894 - JI: 0.1372 - val_loss: 0.0928 - val_JI: 0.1332\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0891 - JI: 0.1397 - val_loss: 0.0939 - val_JI: 0.1253\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0892 - JI: 0.1404 - val_loss: 0.0949 - val_JI: 0.1107\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0887 - JI: 0.1426 - val_loss: 0.0960 - val_JI: 0.1432\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0886 - JI: 0.1440 - val_loss: 0.0933 - val_JI: 0.1431\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0880 - JI: 0.1482 - val_loss: 0.0956 - val_JI: 0.1179\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0891 - JI: 0.1425 - val_loss: 0.0933 - val_JI: 0.1272\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0876 - JI: 0.1512 - val_loss: 0.0936 - val_JI: 0.1344\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0876 - JI: 0.1508 - val_loss: 0.0930 - val_JI: 0.1373\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0870 - JI: 0.1554 - val_loss: 0.0947 - val_JI: 0.1228\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0873 - JI: 0.1560 - val_loss: 0.0931 - val_JI: 0.1367\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0865 - JI: 0.1596 - val_loss: 0.0928 - val_JI: 0.1436\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0861 - JI: 0.1618 - val_loss: 0.0929 - val_JI: 0.1358\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0866 - JI: 0.1610 - val_loss: 0.0931 - val_JI: 0.1349\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0858 - JI: 0.1653 - val_loss: 0.0929 - val_JI: 0.1273\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0857 - JI: 0.1668 - val_loss: 0.0937 - val_JI: 0.1351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0857 - JI: 0.1661 - val_loss: 0.0934 - val_JI: 0.1416\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0851 - JI: 0.1717 - val_loss: 0.0940 - val_JI: 0.1147\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0854 - JI: 0.1697 - val_loss: 0.0947 - val_JI: 0.1320\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0847 - JI: 0.1734 - val_loss: 0.0970 - val_JI: 0.1182\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0852 - JI: 0.1722 - val_loss: 0.0956 - val_JI: 0.1321\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0845 - JI: 0.1760 - val_loss: 0.0939 - val_JI: 0.1266\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0841 - JI: 0.1801 - val_loss: 0.0962 - val_JI: 0.1278\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0838 - JI: 0.1823 - val_loss: 0.0945 - val_JI: 0.1387\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0856 - JI: 0.1747 - val_loss: 0.0942 - val_JI: 0.1476\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.1549 - JI: 0.1683 - val_loss: 0.0966 - val_JI: 0.1235\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1293 - JI: 0.1593 - val_loss: 0.0957 - val_JI: 0.1382\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0943 - JI: 0.1663 - val_loss: 0.0961 - val_JI: 0.1276\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0944 - JI: 0.1621 - val_loss: 0.0942 - val_JI: 0.1329\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0913 - JI: 0.1721 - val_loss: 0.0944 - val_JI: 0.1459\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0870 - JI: 0.1753 - val_loss: 0.0996 - val_JI: 0.1149\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0852 - JI: 0.1772 - val_loss: 0.0957 - val_JI: 0.1457\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0842 - JI: 0.1827 - val_loss: 0.0949 - val_JI: 0.1313\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0831 - JI: 0.1885 - val_loss: 0.0948 - val_JI: 0.1389\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0826 - JI: 0.1915 - val_loss: 0.0956 - val_JI: 0.1303\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0821 - JI: 0.1951 - val_loss: 0.0960 - val_JI: 0.1436\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0817 - JI: 0.1984 - val_loss: 0.0954 - val_JI: 0.1438\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0814 - JI: 0.2004 - val_loss: 0.0963 - val_JI: 0.1464\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0814 - JI: 0.2014 - val_loss: 0.0960 - val_JI: 0.1393\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0818 - JI: 0.1986 - val_loss: 0.0967 - val_JI: 0.1355\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0809 - JI: 0.2052 - val_loss: 0.0966 - val_JI: 0.1364\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0810 - JI: 0.2058 - val_loss: 0.0964 - val_JI: 0.1403\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0798 - JI: 0.2125 - val_loss: 0.0974 - val_JI: 0.1373\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0803 - JI: 0.2131 - val_loss: 0.0987 - val_JI: 0.1423\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0820 - JI: 0.2105 - val_loss: 0.0981 - val_JI: 0.1407\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0902 - JI: 0.2143 - val_loss: 0.1015 - val_JI: 0.1385\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0942 - JI: 0.2004 - val_loss: 0.0987 - val_JI: 0.1373\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0811 - JI: 0.2066 - val_loss: 0.0983 - val_JI: 0.1446\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0799 - JI: 0.2151 - val_loss: 0.0987 - val_JI: 0.1382\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0784 - JI: 0.2246 - val_loss: 0.0985 - val_JI: 0.1333\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0785 - JI: 0.2259 - val_loss: 0.1017 - val_JI: 0.1475\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0789 - JI: 0.2223 - val_loss: 0.0997 - val_JI: 0.1266\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0776 - JI: 0.2316 - val_loss: 0.1008 - val_JI: 0.1443\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0770 - JI: 0.2364 - val_loss: 0.1011 - val_JI: 0.1429\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0793 - JI: 0.2257 - val_loss: 0.1017 - val_JI: 0.1444\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.2277 - val_loss: 0.1040 - val_JI: 0.1288\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0804 - JI: 0.2317 - val_loss: 0.1016 - val_JI: 0.1374\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0784 - JI: 0.2371 - val_loss: 0.1009 - val_JI: 0.1373\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0761 - JI: 0.2435 - val_loss: 0.1024 - val_JI: 0.1488\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0776 - JI: 0.2495 - val_loss: 0.1033 - val_JI: 0.1274\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0753 - JI: 0.2501 - val_loss: 0.1031 - val_JI: 0.1480\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0746 - JI: 0.2548 - val_loss: 0.1025 - val_JI: 0.1383\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0743 - JI: 0.2576 - val_loss: 0.1037 - val_JI: 0.1365\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0754 - JI: 0.2507 - val_loss: 0.1052 - val_JI: 0.1418\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0741 - JI: 0.2596 - val_loss: 0.1047 - val_JI: 0.1453\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0734 - JI: 0.2650 - val_loss: 0.1059 - val_JI: 0.1279\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0726 - JI: 0.2708 - val_loss: 0.1068 - val_JI: 0.1452\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0738 - JI: 0.2631 - val_loss: 0.1072 - val_JI: 0.1412\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0735 - JI: 0.2653 - val_loss: 0.1068 - val_JI: 0.1386\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0744 - JI: 0.2673 - val_loss: 0.1067 - val_JI: 0.1425\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0788 - JI: 0.2669 - val_loss: 0.1073 - val_JI: 0.1392\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0751 - JI: 0.2687 - val_loss: 0.1082 - val_JI: 0.1440\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0721 - JI: 0.2767 - val_loss: 0.1087 - val_JI: 0.1364\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0732 - JI: 0.2708 - val_loss: 0.1079 - val_JI: 0.1326\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0722 - JI: 0.2745 - val_loss: 0.1094 - val_JI: 0.1331\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0708 - JI: 0.2866 - val_loss: 0.1102 - val_JI: 0.1371\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0703 - JI: 0.2902 - val_loss: 0.1107 - val_JI: 0.1383\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0717 - JI: 0.2904 - val_loss: 0.1116 - val_JI: 0.1349\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0702 - JI: 0.2909 - val_loss: 0.1122 - val_JI: 0.1401\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0708 - JI: 0.2948 - val_loss: 0.1128 - val_JI: 0.1364\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0697 - JI: 0.2954 - val_loss: 0.1151 - val_JI: 0.1273\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0710 - JI: 0.2859 - val_loss: 0.1143 - val_JI: 0.1371\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0696 - JI: 0.2966 - val_loss: 0.1152 - val_JI: 0.1356\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0721 - JI: 0.2835 - val_loss: 0.1127 - val_JI: 0.1437\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0690 - JI: 0.2999 - val_loss: 0.1163 - val_JI: 0.1392\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0683 - JI: 0.3063 - val_loss: 0.1179 - val_JI: 0.1401\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0677 - JI: 0.3115 - val_loss: 0.1188 - val_JI: 0.1374\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0672 - JI: 0.3150 - val_loss: 0.1173 - val_JI: 0.1358\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0678 - JI: 0.3111 - val_loss: 0.1206 - val_JI: 0.1404\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0671 - JI: 0.3163 - val_loss: 0.1192 - val_JI: 0.1390\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0669 - JI: 0.3181 - val_loss: 0.1188 - val_JI: 0.1364\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0663 - JI: 0.3219 - val_loss: 0.1215 - val_JI: 0.1303\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0660 - JI: 0.3244 - val_loss: 0.1220 - val_JI: 0.1393\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0661 - JI: 0.3247 - val_loss: 0.1215 - val_JI: 0.1343\n",
      "Epoch 133/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0668 - JI: 0.3184 - val_loss: 0.1214 - val_JI: 0.1336\n",
      "Epoch 134/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0657 - JI: 0.3267 - val_loss: 0.1245 - val_JI: 0.1287\n",
      "Epoch 135/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0654 - JI: 0.3297 - val_loss: 0.1235 - val_JI: 0.1316\n",
      "Epoch 136/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0652 - JI: 0.3308 - val_loss: 0.1264 - val_JI: 0.1300\n",
      "Epoch 137/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0650 - JI: 0.3331 - val_loss: 0.1269 - val_JI: 0.1299\n",
      "Epoch 138/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0646 - JI: 0.3360 - val_loss: 0.1274 - val_JI: 0.1360\n",
      "Epoch 139/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0640 - JI: 0.3407 - val_loss: 0.1271 - val_JI: 0.1330\n",
      "Epoch 140/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0639 - JI: 0.3418 - val_loss: 0.1288 - val_JI: 0.1392\n",
      "Epoch 141/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0638 - JI: 0.3438 - val_loss: 0.1276 - val_JI: 0.1335\n",
      "Epoch 142/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0640 - JI: 0.3410 - val_loss: 0.1275 - val_JI: 0.1357\n",
      "Epoch 143/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0642 - JI: 0.3383 - val_loss: 0.1294 - val_JI: 0.1334\n",
      "Epoch 144/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0635 - JI: 0.3464 - val_loss: 0.1323 - val_JI: 0.1399\n",
      "Epoch 145/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0631 - JI: 0.3478 - val_loss: 0.1332 - val_JI: 0.1375\n",
      "Epoch 146/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0631 - JI: 0.3477 - val_loss: 0.1316 - val_JI: 0.1380\n",
      "Epoch 147/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0623 - JI: 0.3554 - val_loss: 0.1323 - val_JI: 0.1344\n",
      "Epoch 148/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0623 - JI: 0.3549 - val_loss: 0.1338 - val_JI: 0.1368\n",
      "Epoch 149/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0631 - JI: 0.3501 - val_loss: 0.1346 - val_JI: 0.1294\n",
      "Epoch 150/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0625 - JI: 0.3527 - val_loss: 0.1386 - val_JI: 0.1245\n",
      "Epoch 151/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0621 - JI: 0.3561 - val_loss: 0.1355 - val_JI: 0.1348\n",
      "Epoch 152/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0613 - JI: 0.3626 - val_loss: 0.1371 - val_JI: 0.1351\n",
      "Epoch 153/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0613 - JI: 0.3638 - val_loss: 0.1370 - val_JI: 0.1354\n",
      "Epoch 154/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0618 - JI: 0.3590 - val_loss: 0.1381 - val_JI: 0.1340\n",
      "Epoch 155/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0614 - JI: 0.3623 - val_loss: 0.1379 - val_JI: 0.1409\n",
      "Epoch 156/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0605 - JI: 0.3692 - val_loss: 0.1392 - val_JI: 0.1348\n",
      "Epoch 157/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0613 - JI: 0.3634 - val_loss: 0.1423 - val_JI: 0.1340\n",
      "Epoch 158/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0601 - JI: 0.3724 - val_loss: 0.1461 - val_JI: 0.1298\n",
      "Epoch 159/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0600 - JI: 0.3734 - val_loss: 0.1409 - val_JI: 0.1295\n",
      "Epoch 160/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0608 - JI: 0.3678 - val_loss: 0.1432 - val_JI: 0.1339\n",
      "Epoch 161/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0606 - JI: 0.3693 - val_loss: 0.1453 - val_JI: 0.1330\n",
      "Epoch 162/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0593 - JI: 0.3786 - val_loss: 0.1462 - val_JI: 0.1312\n",
      "Epoch 163/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0601 - JI: 0.3731 - val_loss: 0.1420 - val_JI: 0.1323\n",
      "Epoch 164/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0604 - JI: 0.3705 - val_loss: 0.1474 - val_JI: 0.1382\n",
      "Epoch 165/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0594 - JI: 0.3783 - val_loss: 0.1446 - val_JI: 0.1300\n",
      "Epoch 166/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0594 - JI: 0.3791 - val_loss: 0.1482 - val_JI: 0.1352\n",
      "Epoch 167/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0589 - JI: 0.3817 - val_loss: 0.1494 - val_JI: 0.1328\n",
      "Epoch 168/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0583 - JI: 0.3875 - val_loss: 0.1501 - val_JI: 0.1223\n",
      "Epoch 169/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0585 - JI: 0.3863 - val_loss: 0.1493 - val_JI: 0.1347\n",
      "Epoch 170/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0589 - JI: 0.3822 - val_loss: 0.1518 - val_JI: 0.1269\n",
      "Epoch 171/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0581 - JI: 0.3887 - val_loss: 0.1511 - val_JI: 0.1311\n",
      "Epoch 172/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0629 - JI: 0.3606 - val_loss: 0.1523 - val_JI: 0.1292\n",
      "Epoch 173/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0668 - JI: 0.3538 - val_loss: 0.1446 - val_JI: 0.1303\n",
      "Epoch 174/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0984 - JI: 0.3074 - val_loss: 0.1396 - val_JI: 0.1319\n",
      "Epoch 175/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0802 - JI: 0.3215 - val_loss: 0.1479 - val_JI: 0.1289\n",
      "Epoch 176/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0632 - JI: 0.3524 - val_loss: 0.1488 - val_JI: 0.1276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0614 - JI: 0.3622 - val_loss: 0.1469 - val_JI: 0.1262\n",
      "Epoch 178/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0595 - JI: 0.3767 - val_loss: 0.1514 - val_JI: 0.1364\n",
      "Epoch 179/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0588 - JI: 0.3835 - val_loss: 0.1520 - val_JI: 0.1364\n",
      "Epoch 180/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0583 - JI: 0.3873 - val_loss: 0.1551 - val_JI: 0.1283\n",
      "Epoch 181/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0579 - JI: 0.3899 - val_loss: 0.1552 - val_JI: 0.1273\n",
      "Epoch 182/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0580 - JI: 0.3897 - val_loss: 0.1555 - val_JI: 0.1308\n",
      "Epoch 183/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0579 - JI: 0.3905 - val_loss: 0.1566 - val_JI: 0.1319\n",
      "Epoch 184/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0572 - JI: 0.3964 - val_loss: 0.1579 - val_JI: 0.1295\n",
      "Epoch 185/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0574 - JI: 0.3951 - val_loss: 0.1572 - val_JI: 0.1306\n",
      "Epoch 186/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0570 - JI: 0.3974 - val_loss: 0.1570 - val_JI: 0.1359\n",
      "Epoch 187/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0570 - JI: 0.3976 - val_loss: 0.1598 - val_JI: 0.1324\n",
      "Epoch 188/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0572 - JI: 0.3957 - val_loss: 0.1593 - val_JI: 0.1345\n",
      "Epoch 189/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0566 - JI: 0.4009 - val_loss: 0.1604 - val_JI: 0.1296\n",
      "Epoch 190/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0564 - JI: 0.4029 - val_loss: 0.1617 - val_JI: 0.1321\n",
      "Epoch 191/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0561 - JI: 0.4057 - val_loss: 0.1616 - val_JI: 0.1320\n",
      "Epoch 192/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0562 - JI: 0.4047 - val_loss: 0.1658 - val_JI: 0.1308\n",
      "Epoch 193/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0566 - JI: 0.4013 - val_loss: 0.1617 - val_JI: 0.1327\n",
      "Epoch 194/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0557 - JI: 0.4089 - val_loss: 0.1646 - val_JI: 0.1320\n",
      "Epoch 195/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0556 - JI: 0.4094 - val_loss: 0.1668 - val_JI: 0.1310\n",
      "Epoch 196/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0552 - JI: 0.4125 - val_loss: 0.1668 - val_JI: 0.1285\n",
      "Epoch 197/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0550 - JI: 0.4141 - val_loss: 0.1712 - val_JI: 0.1326\n",
      "Epoch 198/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0551 - JI: 0.4143 - val_loss: 0.1706 - val_JI: 0.1274\n",
      "Epoch 199/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0558 - JI: 0.4072 - val_loss: 0.1687 - val_JI: 0.1291\n",
      "Epoch 200/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0593 - JI: 0.3832 - val_loss: 0.1672 - val_JI: 0.1268\n",
      "Epoch 201/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0800 - JI: 0.3471 - val_loss: 0.1600 - val_JI: 0.1298\n",
      "Epoch 202/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0626 - JI: 0.3648 - val_loss: 0.1630 - val_JI: 0.1272\n",
      "Epoch 203/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0613 - JI: 0.3674 - val_loss: 0.1641 - val_JI: 0.1309\n",
      "Epoch 204/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0582 - JI: 0.3895 - val_loss: 0.1651 - val_JI: 0.1297\n",
      "Epoch 205/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0571 - JI: 0.3965 - val_loss: 0.1660 - val_JI: 0.1296\n",
      "Epoch 206/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0561 - JI: 0.4040 - val_loss: 0.1679 - val_JI: 0.1304\n",
      "Epoch 207/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0560 - JI: 0.4044 - val_loss: 0.1697 - val_JI: 0.1324\n",
      "Epoch 208/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0555 - JI: 0.4104 - val_loss: 0.1691 - val_JI: 0.1254\n",
      "Epoch 209/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0551 - JI: 0.4137 - val_loss: 0.1712 - val_JI: 0.1286\n",
      "Epoch 210/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0547 - JI: 0.4161 - val_loss: 0.1689 - val_JI: 0.1285\n",
      "Epoch 211/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0550 - JI: 0.4140 - val_loss: 0.1715 - val_JI: 0.1313\n",
      "Epoch 212/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0546 - JI: 0.4170 - val_loss: 0.1733 - val_JI: 0.1290\n",
      "Epoch 213/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0542 - JI: 0.4212 - val_loss: 0.1722 - val_JI: 0.1286\n",
      "Epoch 214/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0538 - JI: 0.4240 - val_loss: 0.1765 - val_JI: 0.1282\n",
      "Epoch 215/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0541 - JI: 0.4221 - val_loss: 0.1736 - val_JI: 0.1297\n",
      "Epoch 216/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0543 - JI: 0.4205 - val_loss: 0.1783 - val_JI: 0.1280\n",
      "Epoch 217/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0546 - JI: 0.4179 - val_loss: 0.1752 - val_JI: 0.1270\n",
      "Epoch 218/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0549 - JI: 0.4149 - val_loss: 0.1783 - val_JI: 0.1267\n",
      "Epoch 219/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0535 - JI: 0.4267 - val_loss: 0.1801 - val_JI: 0.1289\n",
      "Epoch 220/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0537 - JI: 0.4255 - val_loss: 0.1793 - val_JI: 0.1268\n",
      "Epoch 221/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0534 - JI: 0.4265 - val_loss: 0.1853 - val_JI: 0.1255\n",
      "Epoch 222/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0551 - JI: 0.4131 - val_loss: 0.1838 - val_JI: 0.1304\n",
      "Epoch 223/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0527 - JI: 0.4331 - val_loss: 0.1814 - val_JI: 0.1302\n",
      "Epoch 224/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0527 - JI: 0.4330 - val_loss: 0.1820 - val_JI: 0.1285\n",
      "Epoch 225/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0523 - JI: 0.4367 - val_loss: 0.1822 - val_JI: 0.1299\n",
      "Epoch 226/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0524 - JI: 0.4355 - val_loss: 0.1830 - val_JI: 0.1279\n",
      "Epoch 227/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0532 - JI: 0.4296 - val_loss: 0.1804 - val_JI: 0.1287\n",
      "Epoch 228/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0522 - JI: 0.4357 - val_loss: 0.1841 - val_JI: 0.1246\n",
      "Epoch 229/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0526 - JI: 0.4347 - val_loss: 0.1857 - val_JI: 0.1298\n",
      "Epoch 230/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0524 - JI: 0.4366 - val_loss: 0.1871 - val_JI: 0.1256\n",
      "Epoch 231/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0521 - JI: 0.4374 - val_loss: 0.1889 - val_JI: 0.1304\n",
      "Epoch 232/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0523 - JI: 0.4366 - val_loss: 0.1877 - val_JI: 0.1300\n",
      "Epoch 233/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0517 - JI: 0.4406 - val_loss: 0.1900 - val_JI: 0.1300\n",
      "Epoch 234/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0515 - JI: 0.4434 - val_loss: 0.1883 - val_JI: 0.1231\n",
      "Epoch 235/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0524 - JI: 0.4350 - val_loss: 0.1934 - val_JI: 0.1267\n",
      "Epoch 236/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0520 - JI: 0.4381 - val_loss: 0.1902 - val_JI: 0.1238\n",
      "Epoch 237/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0516 - JI: 0.4421 - val_loss: 0.1966 - val_JI: 0.1228\n",
      "Epoch 238/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0511 - JI: 0.4461 - val_loss: 0.1952 - val_JI: 0.1276\n",
      "Epoch 239/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0512 - JI: 0.4447 - val_loss: 0.1925 - val_JI: 0.1252\n",
      "Epoch 240/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0509 - JI: 0.4477 - val_loss: 0.1981 - val_JI: 0.1287\n",
      "Epoch 241/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0521 - JI: 0.4379 - val_loss: 0.1955 - val_JI: 0.1286\n",
      "Epoch 242/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0516 - JI: 0.4414 - val_loss: 0.1999 - val_JI: 0.1268\n",
      "Epoch 243/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0509 - JI: 0.4467 - val_loss: 0.1910 - val_JI: 0.1289\n",
      "Epoch 244/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0506 - JI: 0.4499 - val_loss: 0.1954 - val_JI: 0.1346\n",
      "Epoch 245/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0512 - JI: 0.4448 - val_loss: 0.2029 - val_JI: 0.1245\n",
      "Epoch 246/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0505 - JI: 0.4503 - val_loss: 0.1974 - val_JI: 0.1254\n",
      "Epoch 247/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0506 - JI: 0.4508 - val_loss: 0.2015 - val_JI: 0.1198\n",
      "Epoch 248/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0503 - JI: 0.4528 - val_loss: 0.2015 - val_JI: 0.1274\n",
      "Epoch 249/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0497 - JI: 0.4578 - val_loss: 0.2022 - val_JI: 0.1253\n",
      "Epoch 250/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0495 - JI: 0.4602 - val_loss: 0.2013 - val_JI: 0.1272\n",
      "Epoch 251/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0497 - JI: 0.4579 - val_loss: 0.2075 - val_JI: 0.1252\n",
      "Epoch 252/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0524 - JI: 0.4364 - val_loss: 0.2006 - val_JI: 0.1274\n",
      "Epoch 253/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0508 - JI: 0.4490 - val_loss: 0.2082 - val_JI: 0.1251\n",
      "Epoch 254/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0501 - JI: 0.4549 - val_loss: 0.2071 - val_JI: 0.1202\n",
      "Epoch 255/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0496 - JI: 0.4586 - val_loss: 0.2054 - val_JI: 0.1239\n",
      "Epoch 256/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0496 - JI: 0.4592 - val_loss: 0.2094 - val_JI: 0.1273\n",
      "Epoch 257/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0490 - JI: 0.4640 - val_loss: 0.2095 - val_JI: 0.1267\n",
      "Epoch 258/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0496 - JI: 0.4591 - val_loss: 0.2119 - val_JI: 0.1260\n",
      "Epoch 259/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0497 - JI: 0.4571 - val_loss: 0.2113 - val_JI: 0.1266\n",
      "Epoch 260/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0503 - JI: 0.4512 - val_loss: 0.2106 - val_JI: 0.1261\n",
      "Epoch 261/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0497 - JI: 0.4575 - val_loss: 0.2152 - val_JI: 0.1270\n",
      "Epoch 262/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0491 - JI: 0.4626 - val_loss: 0.2130 - val_JI: 0.1216\n",
      "Epoch 263/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0493 - JI: 0.4616 - val_loss: 0.2118 - val_JI: 0.1197\n",
      "Epoch 264/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0492 - JI: 0.4616 - val_loss: 0.2104 - val_JI: 0.1279\n",
      "Epoch 265/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0490 - JI: 0.4642 - val_loss: 0.2182 - val_JI: 0.1244\n",
      "Epoch 266/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0486 - JI: 0.4669 - val_loss: 0.2183 - val_JI: 0.1248\n",
      "Epoch 267/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0492 - JI: 0.4606 - val_loss: 0.2180 - val_JI: 0.1248\n",
      "Epoch 268/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0486 - JI: 0.4674 - val_loss: 0.2203 - val_JI: 0.1222\n",
      "Epoch 269/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0496 - JI: 0.4586 - val_loss: 0.2172 - val_JI: 0.1244\n",
      "Epoch 270/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0484 - JI: 0.4689 - val_loss: 0.2197 - val_JI: 0.1249\n",
      "Epoch 271/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0488 - JI: 0.4645 - val_loss: 0.2181 - val_JI: 0.1188\n",
      "Epoch 272/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0494 - JI: 0.4603 - val_loss: 0.2158 - val_JI: 0.1205\n",
      "Epoch 273/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0483 - JI: 0.4693 - val_loss: 0.2223 - val_JI: 0.1264\n",
      "Epoch 274/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0479 - JI: 0.4729 - val_loss: 0.2150 - val_JI: 0.1275\n",
      "Epoch 275/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0484 - JI: 0.4686 - val_loss: 0.2167 - val_JI: 0.1264\n",
      "Epoch 276/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0476 - JI: 0.4766 - val_loss: 0.2233 - val_JI: 0.1213\n",
      "Epoch 277/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0477 - JI: 0.4747 - val_loss: 0.2271 - val_JI: 0.1260\n",
      "Epoch 278/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0498 - JI: 0.4578 - val_loss: 0.2204 - val_JI: 0.1233\n",
      "Epoch 279/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0478 - JI: 0.4735 - val_loss: 0.2321 - val_JI: 0.1233\n",
      "Epoch 280/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0481 - JI: 0.4707 - val_loss: 0.2242 - val_JI: 0.1226\n",
      "Epoch 281/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0481 - JI: 0.4706 - val_loss: 0.2264 - val_JI: 0.1199\n",
      "Epoch 282/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0479 - JI: 0.4721 - val_loss: 0.2268 - val_JI: 0.1219\n",
      "Epoch 283/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0477 - JI: 0.4735 - val_loss: 0.2283 - val_JI: 0.1242\n",
      "Epoch 284/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0478 - JI: 0.4741 - val_loss: 0.2316 - val_JI: 0.1241\n",
      "Epoch 285/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0475 - JI: 0.4772 - val_loss: 0.2250 - val_JI: 0.1269\n",
      "Epoch 286/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0474 - JI: 0.4777 - val_loss: 0.2229 - val_JI: 0.1218\n",
      "Epoch 287/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0487 - JI: 0.4661 - val_loss: 0.2352 - val_JI: 0.1223\n",
      "Epoch 288/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0482 - JI: 0.4699 - val_loss: 0.2258 - val_JI: 0.1253\n",
      "Epoch 289/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0518 - JI: 0.4429 - val_loss: 0.2308 - val_JI: 0.1227\n",
      "Epoch 290/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0468 - JI: 0.4827 - val_loss: 0.2316 - val_JI: 0.1220\n",
      "Epoch 291/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0464 - JI: 0.4861 - val_loss: 0.2358 - val_JI: 0.1237\n",
      "Epoch 292/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0461 - JI: 0.4894 - val_loss: 0.2293 - val_JI: 0.1245\n",
      "Epoch 293/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0467 - JI: 0.4824 - val_loss: 0.2306 - val_JI: 0.1215\n",
      "Epoch 294/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0465 - JI: 0.4848 - val_loss: 0.2368 - val_JI: 0.1248\n",
      "Epoch 295/1000\n",
      " 57/120 [=============>................] - ETA: 0s - loss: 0.0465 - JI: 0.4855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PINKPI~1\\AppData\\Local\\Temp/ipykernel_9612/3177698928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_deep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\PINKPI~1\\AppData\\Local\\Temp/ipykernel_9612/2974927649.py\u001b[0m in \u001b[0;36mtrain_deep\u001b[1;34m(X_train, y_train, X_test, y_test, layer_num, callbacks_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_deep(X_train,y_train,X_test,y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc9fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,002,732\n",
      "Trainable params: 2,002,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.2085 - JI: 0.0149 - val_loss: 0.1121 - val_JI: 0.0032\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1168 - JI: 0.0086 - val_loss: 0.1108 - val_JI: 0.0061\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1171 - JI: 0.0114 - val_loss: 0.1101 - val_JI: 0.0060\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1109 - JI: 0.0211 - val_loss: 0.1076 - val_JI: 0.0309\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1210 - JI: 0.0358 - val_loss: 0.1075 - val_JI: 0.0404\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1292 - JI: 0.0457 - val_loss: 0.1111 - val_JI: 0.0483\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.1076 - JI: 0.0450 - val_loss: 0.1050 - val_JI: 0.0492\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1106 - JI: 0.0536 - val_loss: 0.1058 - val_JI: 0.0559\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.1049 - JI: 0.0583 - val_loss: 0.1035 - val_JI: 0.0721\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1086 - JI: 0.0635 - val_loss: 0.1034 - val_JI: 0.0727\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1028 - JI: 0.0667 - val_loss: 0.1009 - val_JI: 0.0785\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1013 - JI: 0.0746 - val_loss: 0.1003 - val_JI: 0.0818\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1014 - JI: 0.0811 - val_loss: 0.1000 - val_JI: 0.0872\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1052 - JI: 0.0805 - val_loss: 0.1034 - val_JI: 0.0914\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0997 - JI: 0.0826 - val_loss: 0.1003 - val_JI: 0.0760\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1014 - JI: 0.0831 - val_loss: 0.1012 - val_JI: 0.0684\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.1005 - JI: 0.0864 - val_loss: 0.0972 - val_JI: 0.0816\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0970 - JI: 0.0907 - val_loss: 0.0966 - val_JI: 0.1005\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0967 - JI: 0.0928 - val_loss: 0.0974 - val_JI: 0.0885\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0961 - JI: 0.0952 - val_loss: 0.0984 - val_JI: 0.1057\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0964 - JI: 0.0935 - val_loss: 0.0964 - val_JI: 0.0973\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0954 - JI: 0.0999 - val_loss: 0.0956 - val_JI: 0.0899\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0948 - JI: 0.1008 - val_loss: 0.0957 - val_JI: 0.1038\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0945 - JI: 0.1028 - val_loss: 0.0952 - val_JI: 0.1010\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0943 - JI: 0.1048 - val_loss: 0.0950 - val_JI: 0.1055\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0948 - JI: 0.1028 - val_loss: 0.0979 - val_JI: 0.0739\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0944 - JI: 0.1045 - val_loss: 0.0972 - val_JI: 0.0988\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0937 - JI: 0.1077 - val_loss: 0.0959 - val_JI: 0.1058\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0932 - JI: 0.1104 - val_loss: 0.0964 - val_JI: 0.1201\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0928 - JI: 0.1139 - val_loss: 0.0944 - val_JI: 0.1043\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0929 - JI: 0.1145 - val_loss: 0.0952 - val_JI: 0.1087\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0924 - JI: 0.1175 - val_loss: 0.0963 - val_JI: 0.0997\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0924 - JI: 0.1180 - val_loss: 0.0950 - val_JI: 0.1076\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0921 - JI: 0.1198 - val_loss: 0.0943 - val_JI: 0.1185\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0939 - JI: 0.1195 - val_loss: 0.0958 - val_JI: 0.0981\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.1548 - JI: 0.1172 - val_loss: 0.0952 - val_JI: 0.1175\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0925 - JI: 0.1185 - val_loss: 0.0956 - val_JI: 0.0909\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0994 - JI: 0.1155 - val_loss: 0.0943 - val_JI: 0.1108\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0916 - JI: 0.1243 - val_loss: 0.0936 - val_JI: 0.1178\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0992 - JI: 0.1211 - val_loss: 0.0946 - val_JI: 0.1046\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.1106 - JI: 0.1233 - val_loss: 0.0956 - val_JI: 0.1254\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0962 - JI: 0.1221 - val_loss: 0.0940 - val_JI: 0.1278\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0909 - JI: 0.1273 - val_loss: 0.0939 - val_JI: 0.1220\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0904 - JI: 0.1308 - val_loss: 0.0934 - val_JI: 0.1264\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0904 - JI: 0.1305 - val_loss: 0.0931 - val_JI: 0.1195\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0902 - JI: 0.1324 - val_loss: 0.0941 - val_JI: 0.1209\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0896 - JI: 0.1357 - val_loss: 0.0927 - val_JI: 0.1309\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0900 - JI: 0.1353 - val_loss: 0.0936 - val_JI: 0.1240\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0894 - JI: 0.1371 - val_loss: 0.0934 - val_JI: 0.1226\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0891 - JI: 0.1392 - val_loss: 0.0956 - val_JI: 0.1299\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0894 - JI: 0.1389 - val_loss: 0.0936 - val_JI: 0.1249\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0887 - JI: 0.1421 - val_loss: 0.0935 - val_JI: 0.1399\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0888 - JI: 0.1417 - val_loss: 0.0926 - val_JI: 0.1258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0885 - JI: 0.1440 - val_loss: 0.0944 - val_JI: 0.1203\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0883 - JI: 0.1461 - val_loss: 0.0936 - val_JI: 0.1194\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0879 - JI: 0.1481 - val_loss: 0.0926 - val_JI: 0.1206\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0880 - JI: 0.1479 - val_loss: 0.0931 - val_JI: 0.1290\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0878 - JI: 0.1506 - val_loss: 0.0930 - val_JI: 0.1351\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0873 - JI: 0.1534 - val_loss: 0.0930 - val_JI: 0.1350\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0879 - JI: 0.1523 - val_loss: 0.0940 - val_JI: 0.1346\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0888 - JI: 0.1520 - val_loss: 0.0931 - val_JI: 0.1315\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0869 - JI: 0.1565 - val_loss: 0.0959 - val_JI: 0.1170\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0868 - JI: 0.1569 - val_loss: 0.0936 - val_JI: 0.1276\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0867 - JI: 0.1589 - val_loss: 0.0943 - val_JI: 0.1163\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0860 - JI: 0.1620 - val_loss: 0.0939 - val_JI: 0.1326\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0861 - JI: 0.1628 - val_loss: 0.0934 - val_JI: 0.1460\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0858 - JI: 0.1649 - val_loss: 0.0957 - val_JI: 0.1136\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0857 - JI: 0.1653 - val_loss: 0.0932 - val_JI: 0.1317\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0854 - JI: 0.1674 - val_loss: 0.0930 - val_JI: 0.1368\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0848 - JI: 0.1711 - val_loss: 0.0939 - val_JI: 0.1449\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0849 - JI: 0.1717 - val_loss: 0.0936 - val_JI: 0.1469\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0852 - JI: 0.1701 - val_loss: 0.0978 - val_JI: 0.1392\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0851 - JI: 0.1707 - val_loss: 0.0942 - val_JI: 0.1370\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0853 - JI: 0.1693 - val_loss: 0.0936 - val_JI: 0.1258\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0849 - JI: 0.1723 - val_loss: 0.0938 - val_JI: 0.1337\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0840 - JI: 0.1778 - val_loss: 0.0946 - val_JI: 0.1315\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0848 - JI: 0.1786 - val_loss: 0.0944 - val_JI: 0.1407\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0839 - JI: 0.1804 - val_loss: 0.0943 - val_JI: 0.1320\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0837 - JI: 0.1806 - val_loss: 0.0944 - val_JI: 0.1313\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0832 - JI: 0.1844 - val_loss: 0.0946 - val_JI: 0.1439\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0845 - JI: 0.1760 - val_loss: 0.0951 - val_JI: 0.1457\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0831 - JI: 0.1853 - val_loss: 0.0950 - val_JI: 0.1378\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0822 - JI: 0.1925 - val_loss: 0.0950 - val_JI: 0.1516\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0831 - JI: 0.1875 - val_loss: 0.0949 - val_JI: 0.1283\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.1274 - JI: 0.1870 - val_loss: 0.0951 - val_JI: 0.1450\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0822 - JI: 0.1926 - val_loss: 0.0950 - val_JI: 0.1344\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0815 - JI: 0.1980 - val_loss: 0.0948 - val_JI: 0.1452\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0816 - JI: 0.1979 - val_loss: 0.0954 - val_JI: 0.1363\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0814 - JI: 0.1991 - val_loss: 0.0970 - val_JI: 0.1467\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0822 - JI: 0.1978 - val_loss: 0.0973 - val_JI: 0.1295\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0814 - JI: 0.1998 - val_loss: 0.0964 - val_JI: 0.1375\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0804 - JI: 0.2058 - val_loss: 0.0968 - val_JI: 0.1475\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 0.0816 - JI: 0.1981 - val_loss: 0.0969 - val_JI: 0.1464\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0803 - JI: 0.2075 - val_loss: 0.0968 - val_JI: 0.1387\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0797 - JI: 0.2119 - val_loss: 0.0968 - val_JI: 0.1447\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0794 - JI: 0.2149 - val_loss: 0.0990 - val_JI: 0.1410\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0797 - JI: 0.2136 - val_loss: 0.0979 - val_JI: 0.1404\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 19ms/step - loss: 0.0789 - JI: 0.2176 - val_loss: 0.0977 - val_JI: 0.1442\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0786 - JI: 0.2214 - val_loss: 0.0976 - val_JI: 0.1412\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0783 - JI: 0.2224 - val_loss: 0.0987 - val_JI: 0.1512\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0786 - JI: 0.2210 - val_loss: 0.0995 - val_JI: 0.1338\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0779 - JI: 0.2261 - val_loss: 0.0987 - val_JI: 0.1443\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.0784 - JI: 0.2227 - val_loss: 0.0998 - val_JI: 0.1222\n"
     ]
    }
   ],
   "source": [
    "train_deep(X_train,y_train,X_test,y_test,3, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "415c0877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,265,388\n",
      "Trainable params: 2,265,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1772 - JI: 0.0128 - val_loss: 0.1114 - val_JI: 0.0074\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1133 - JI: 0.0095 - val_loss: 0.1121 - val_JI: 0.0079\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1135 - JI: 0.0176 - val_loss: 0.1203 - val_JI: 0.0327\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1125 - JI: 0.0229 - val_loss: 0.1074 - val_JI: 0.0368\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1125 - JI: 0.0404 - val_loss: 0.1070 - val_JI: 0.0592\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1210 - JI: 0.0484 - val_loss: 0.1074 - val_JI: 0.0506\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1077 - JI: 0.0533 - val_loss: 0.1078 - val_JI: 0.0548\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1190 - JI: 0.0575 - val_loss: 0.1071 - val_JI: 0.0685\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1103 - JI: 0.0616 - val_loss: 0.1062 - val_JI: 0.0859\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1076 - JI: 0.0620 - val_loss: 0.1027 - val_JI: 0.0622\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1246 - JI: 0.0644 - val_loss: 0.1019 - val_JI: 0.0631\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1019 - JI: 0.0727 - val_loss: 0.1023 - val_JI: 0.0831\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1025 - JI: 0.0776 - val_loss: 0.1036 - val_JI: 0.0807\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1000 - JI: 0.0790 - val_loss: 0.0997 - val_JI: 0.0787\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1121 - JI: 0.0823 - val_loss: 0.0981 - val_JI: 0.0832\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1127 - JI: 0.0838 - val_loss: 0.0991 - val_JI: 0.0811\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1036 - JI: 0.0875 - val_loss: 0.1015 - val_JI: 0.0779\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0972 - JI: 0.0888 - val_loss: 0.0978 - val_JI: 0.0785\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0991 - JI: 0.0945 - val_loss: 0.0997 - val_JI: 0.0917\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0962 - JI: 0.0955 - val_loss: 0.0993 - val_JI: 0.0760\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0957 - JI: 0.0963 - val_loss: 0.0963 - val_JI: 0.0918\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0981 - JI: 0.0953 - val_loss: 0.0970 - val_JI: 0.0870\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0965 - JI: 0.0944 - val_loss: 0.0959 - val_JI: 0.0988\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0989 - JI: 0.1004 - val_loss: 0.0961 - val_JI: 0.1016\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0947 - JI: 0.1018 - val_loss: 0.0957 - val_JI: 0.1151\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0955 - JI: 0.1056 - val_loss: 0.0954 - val_JI: 0.0906\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0943 - JI: 0.1047 - val_loss: 0.0947 - val_JI: 0.1024\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0961 - JI: 0.1062 - val_loss: 0.0957 - val_JI: 0.0905\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0957 - JI: 0.1064 - val_loss: 0.0947 - val_JI: 0.1010\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0932 - JI: 0.1110 - val_loss: 0.0944 - val_JI: 0.1147\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0934 - JI: 0.1105 - val_loss: 0.0943 - val_JI: 0.1097\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0936 - JI: 0.1114 - val_loss: 0.0951 - val_JI: 0.1076\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0928 - JI: 0.1149 - val_loss: 0.0938 - val_JI: 0.1177\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0920 - JI: 0.1180 - val_loss: 0.0950 - val_JI: 0.1204\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0922 - JI: 0.1177 - val_loss: 0.0951 - val_JI: 0.1132\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0918 - JI: 0.1197 - val_loss: 0.0943 - val_JI: 0.1162\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0924 - JI: 0.1168 - val_loss: 0.0939 - val_JI: 0.1132\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0914 - JI: 0.1214 - val_loss: 0.0949 - val_JI: 0.1184\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0914 - JI: 0.1222 - val_loss: 0.0938 - val_JI: 0.1109\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0910 - JI: 0.1239 - val_loss: 0.0935 - val_JI: 0.1265\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0909 - JI: 0.1271 - val_loss: 0.0935 - val_JI: 0.1169\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0910 - JI: 0.1258 - val_loss: 0.0931 - val_JI: 0.1249\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.0901 - JI: 0.1300 - val_loss: 0.0948 - val_JI: 0.1151\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.0904 - JI: 0.1294 - val_loss: 0.0936 - val_JI: 0.1128\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.0899 - JI: 0.1324 - val_loss: 0.0930 - val_JI: 0.1290\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0898 - JI: 0.1343 - val_loss: 0.0942 - val_JI: 0.1121\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0896 - JI: 0.1348 - val_loss: 0.0933 - val_JI: 0.1165\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0902 - JI: 0.1312 - val_loss: 0.0944 - val_JI: 0.1254\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0895 - JI: 0.1364 - val_loss: 0.0933 - val_JI: 0.1279\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0897 - JI: 0.1357 - val_loss: 0.0967 - val_JI: 0.0888\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0899 - JI: 0.1350 - val_loss: 0.0927 - val_JI: 0.1267\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0901 - JI: 0.1403 - val_loss: 0.0944 - val_JI: 0.1201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0898 - JI: 0.1390 - val_loss: 0.0931 - val_JI: 0.1246\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0897 - JI: 0.1415 - val_loss: 0.0932 - val_JI: 0.1140\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0944 - JI: 0.1387 - val_loss: 0.0932 - val_JI: 0.1326\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0907 - JI: 0.1367 - val_loss: 0.0948 - val_JI: 0.1175\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.0982 - JI: 0.1398 - val_loss: 0.0939 - val_JI: 0.1383\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0893 - JI: 0.1382 - val_loss: 0.0956 - val_JI: 0.1219\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0936 - JI: 0.1334 - val_loss: 0.0926 - val_JI: 0.1302\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0921 - JI: 0.1354 - val_loss: 0.0950 - val_JI: 0.1177\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0906 - JI: 0.1347 - val_loss: 0.0932 - val_JI: 0.1350\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.1078 - JI: 0.1336 - val_loss: 0.0931 - val_JI: 0.1185\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0918 - JI: 0.1382 - val_loss: 0.0939 - val_JI: 0.1175\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0918 - JI: 0.1380 - val_loss: 0.0929 - val_JI: 0.1166\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0904 - JI: 0.1401 - val_loss: 0.0928 - val_JI: 0.1314\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0915 - JI: 0.1425 - val_loss: 0.0955 - val_JI: 0.1061\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0953 - JI: 0.1410 - val_loss: 0.0934 - val_JI: 0.1300\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0901 - JI: 0.1406 - val_loss: 0.0939 - val_JI: 0.1376\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0954 - JI: 0.1436 - val_loss: 0.0956 - val_JI: 0.1227\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0897 - JI: 0.1458 - val_loss: 0.0940 - val_JI: 0.1296\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0881 - JI: 0.1499 - val_loss: 0.0926 - val_JI: 0.1400\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0876 - JI: 0.1505 - val_loss: 0.0929 - val_JI: 0.1155\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0868 - JI: 0.1554 - val_loss: 0.0949 - val_JI: 0.1147\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0870 - JI: 0.1545 - val_loss: 0.0933 - val_JI: 0.1321\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0866 - JI: 0.1566 - val_loss: 0.0934 - val_JI: 0.1395\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0863 - JI: 0.1596 - val_loss: 0.0929 - val_JI: 0.1260\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0867 - JI: 0.1592 - val_loss: 0.0926 - val_JI: 0.1410\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0868 - JI: 0.1607 - val_loss: 0.0933 - val_JI: 0.1342\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0855 - JI: 0.1642 - val_loss: 0.0930 - val_JI: 0.1412\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0853 - JI: 0.1672 - val_loss: 0.0928 - val_JI: 0.1441\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0852 - JI: 0.1679 - val_loss: 0.0933 - val_JI: 0.1376\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0853 - JI: 0.1670 - val_loss: 0.0936 - val_JI: 0.1298\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0851 - JI: 0.1691 - val_loss: 0.0949 - val_JI: 0.1340\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0846 - JI: 0.1717 - val_loss: 0.0929 - val_JI: 0.1347\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0846 - JI: 0.1721 - val_loss: 0.0937 - val_JI: 0.1374\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0843 - JI: 0.1743 - val_loss: 0.0933 - val_JI: 0.1342\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0838 - JI: 0.1776 - val_loss: 0.0940 - val_JI: 0.1301\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0844 - JI: 0.1748 - val_loss: 0.0948 - val_JI: 0.1239\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0835 - JI: 0.1793 - val_loss: 0.0935 - val_JI: 0.1371\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0835 - JI: 0.1810 - val_loss: 0.0943 - val_JI: 0.1378\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0836 - JI: 0.1810 - val_loss: 0.0947 - val_JI: 0.1424\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0829 - JI: 0.1846 - val_loss: 0.0944 - val_JI: 0.1432\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0831 - JI: 0.1841 - val_loss: 0.0954 - val_JI: 0.1325\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0831 - JI: 0.1843 - val_loss: 0.0941 - val_JI: 0.1488\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0897 - JI: 0.1781 - val_loss: 0.0944 - val_JI: 0.1331\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.1015 - JI: 0.1815 - val_loss: 0.0960 - val_JI: 0.1290\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0863 - JI: 0.1807 - val_loss: 0.0957 - val_JI: 0.1342\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0872 - JI: 0.1759 - val_loss: 0.0942 - val_JI: 0.1445\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0832 - JI: 0.1866 - val_loss: 0.0968 - val_JI: 0.1424\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0831 - JI: 0.1886 - val_loss: 0.0949 - val_JI: 0.1340\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0822 - JI: 0.1924 - val_loss: 0.0952 - val_JI: 0.1408\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.1778 - JI: 0.1897 - val_loss: 0.0959 - val_JI: 0.1413\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0880 - JI: 0.1841 - val_loss: 0.0954 - val_JI: 0.1421\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0945 - JI: 0.1871 - val_loss: 0.0968 - val_JI: 0.1316\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0832 - JI: 0.1912 - val_loss: 0.0968 - val_JI: 0.1398\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0828 - JI: 0.1888 - val_loss: 0.0962 - val_JI: 0.1270\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0896 - JI: 0.1933 - val_loss: 0.0956 - val_JI: 0.1353\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0820 - JI: 0.1931 - val_loss: 0.0966 - val_JI: 0.1315\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0810 - JI: 0.2011 - val_loss: 0.0969 - val_JI: 0.1486\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0804 - JI: 0.2057 - val_loss: 0.0971 - val_JI: 0.1376\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0809 - JI: 0.2030 - val_loss: 0.0975 - val_JI: 0.1288\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0801 - JI: 0.2082 - val_loss: 0.0965 - val_JI: 0.1414\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.0796 - JI: 0.2120 - val_loss: 0.0969 - val_JI: 0.1397\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0795 - JI: 0.2134 - val_loss: 0.0978 - val_JI: 0.1424\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0797 - JI: 0.2128 - val_loss: 0.0974 - val_JI: 0.1421\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0789 - JI: 0.2176 - val_loss: 0.0981 - val_JI: 0.1402\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0780 - JI: 0.2231 - val_loss: 0.0987 - val_JI: 0.1420\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0779 - JI: 0.2253 - val_loss: 0.0985 - val_JI: 0.1310\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0776 - JI: 0.2267 - val_loss: 0.0994 - val_JI: 0.1417\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.0769 - JI: 0.2334 - val_loss: 0.0998 - val_JI: 0.1429\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 2s 21ms/step - loss: 0.0770 - JI: 0.2339 - val_loss: 0.0995 - val_JI: 0.1415\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_deep(X_train,y_train,X_test,y_test,4, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ba32e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def deep_model_reg(feature_dim,label_dim, layer_num=1):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    print(\"create model. feature_dim ={}, label_dim ={}\".format(feature_dim, label_dim))\n",
    "    model.add(Dense(512, activation='relu', input_dim=feature_dim))\n",
    "    \n",
    "    for i in range(layer_num):\n",
    "        model.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(Dense(label_dim, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[JI])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc3423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def train_deep_reg(X_train,y_train,X_test,y_test, layer_num=1, callbacks_list = []):\n",
    "    feature_dim = X_train.shape[1]\n",
    "    label_dim = y_train.shape[1]\n",
    "    model = deep_model_reg(feature_dim,label_dim, layer_num)\n",
    "    model.summary()\n",
    "    history = model.fit(X_train,y_train,batch_size=256, epochs=1000,callbacks=callbacks_list,validation_data=(X_test,y_test), verbose=1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44a27014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_61 (Dense)            (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,477,420\n",
      "Trainable params: 1,477,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 29.4558 - JI: 0.0284 - val_loss: 3.4383 - val_JI: 0.0148\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 1.2630 - JI: 0.0088 - val_loss: 0.4945 - val_JI: 3.7131e-04\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.5198 - JI: 0.0037 - val_loss: 0.4131 - val_JI: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4944 - JI: 0.0037 - val_loss: 0.4580 - val_JI: 2.2343e-04\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4666 - JI: 0.0026 - val_loss: 0.4284 - val_JI: 3.2457e-05\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.6719 - JI: 0.0042 - val_loss: 0.4765 - val_JI: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4156 - JI: 1.1183e-04 - val_loss: 0.7508 - val_JI: 8.9948e-04\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.5304 - JI: 0.0010 - val_loss: 0.4038 - val_JI: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4474 - JI: 6.8108e-04 - val_loss: 0.4337 - val_JI: 1.1284e-04\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.5853 - JI: 1.0600e-04 - val_loss: 0.4374 - val_JI: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4281 - JI: 4.1660e-04 - val_loss: 0.4147 - val_JI: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4169 - JI: 4.0681e-04 - val_loss: 0.4679 - val_JI: 7.8846e-05\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4213 - JI: 3.6995e-05 - val_loss: 0.4023 - val_JI: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4152 - JI: 7.5512e-05 - val_loss: 0.4018 - val_JI: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4131 - JI: 3.9436e-05 - val_loss: 0.4684 - val_JI: 3.2410e-05\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4480 - JI: 6.1054e-05 - val_loss: 0.5529 - val_JI: 2.5257e-04\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4364 - JI: 2.4522e-04 - val_loss: 0.4045 - val_JI: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4218 - JI: 4.4289e-04 - val_loss: 0.4182 - val_JI: 3.1997e-05\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4382 - JI: 6.7439e-04 - val_loss: 0.4161 - val_JI: 4.8522e-05\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.9013 - JI: 5.1156e-04 - val_loss: 0.4629 - val_JI: 1.6420e-05\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4398 - JI: 0.0013 - val_loss: 0.4073 - val_JI: 0.0025\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4122 - JI: 0.0014 - val_loss: 0.4025 - val_JI: 9.7578e-04\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4049 - JI: 0.0017 - val_loss: 0.4368 - val_JI: 0.0028\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4225 - JI: 0.0037 - val_loss: 0.4679 - val_JI: 0.0034\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4144 - JI: 0.0053 - val_loss: 0.4695 - val_JI: 0.0063\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4610 - JI: 0.0065 - val_loss: 0.4673 - val_JI: 0.0100\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4135 - JI: 0.0087 - val_loss: 0.3985 - val_JI: 0.0099\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4059 - JI: 0.0103 - val_loss: 0.3987 - val_JI: 0.0095\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4050 - JI: 0.0121 - val_loss: 0.3992 - val_JI: 0.0107\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4005 - JI: 0.0143 - val_loss: 0.4062 - val_JI: 0.0131\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4010 - JI: 0.0155 - val_loss: 0.4000 - val_JI: 0.0120\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.4074 - JI: 0.0177 - val_loss: 0.3989 - val_JI: 0.0184\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.4008 - JI: 0.0189 - val_loss: 0.3981 - val_JI: 0.0188\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4001 - JI: 0.0207 - val_loss: 0.3994 - val_JI: 0.0164\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4001 - JI: 0.0226 - val_loss: 0.3972 - val_JI: 0.0222\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3999 - JI: 0.0238 - val_loss: 0.3983 - val_JI: 0.0271\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3991 - JI: 0.0248 - val_loss: 0.3973 - val_JI: 0.0277\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3995 - JI: 0.0268 - val_loss: 0.4007 - val_JI: 0.0327\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4063 - JI: 0.0283 - val_loss: 0.4032 - val_JI: 0.0296\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4001 - JI: 0.0283 - val_loss: 0.3981 - val_JI: 0.0265\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4012 - JI: 0.0297 - val_loss: 0.3980 - val_JI: 0.0307\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3987 - JI: 0.0307 - val_loss: 0.3979 - val_JI: 0.0301\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3975 - JI: 0.0309 - val_loss: 0.4112 - val_JI: 0.0333\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4047 - JI: 0.0322 - val_loss: 0.4001 - val_JI: 0.0328\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3981 - JI: 0.0334 - val_loss: 0.3989 - val_JI: 0.0329\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3999 - JI: 0.0333 - val_loss: 0.3984 - val_JI: 0.0317\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4067 - JI: 0.0338 - val_loss: 0.4055 - val_JI: 0.0315\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4021 - JI: 0.0343 - val_loss: 0.3979 - val_JI: 0.0307\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3985 - JI: 0.0347 - val_loss: 0.3968 - val_JI: 0.0367\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3997 - JI: 0.0356 - val_loss: 0.3973 - val_JI: 0.0312\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3972 - JI: 0.0355 - val_loss: 0.3962 - val_JI: 0.0403\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3976 - JI: 0.0362 - val_loss: 0.3963 - val_JI: 0.0400\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3982 - JI: 0.0369 - val_loss: 0.3982 - val_JI: 0.0323\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3973 - JI: 0.0373 - val_loss: 0.3952 - val_JI: 0.0295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3969 - JI: 0.0369 - val_loss: 0.3956 - val_JI: 0.0351\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3964 - JI: 0.0371 - val_loss: 0.3978 - val_JI: 0.0371\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3965 - JI: 0.0384 - val_loss: 0.3948 - val_JI: 0.0339\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3967 - JI: 0.0382 - val_loss: 0.3950 - val_JI: 0.0395\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3962 - JI: 0.0392 - val_loss: 0.3961 - val_JI: 0.0391\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3962 - JI: 0.0384 - val_loss: 0.3965 - val_JI: 0.0367\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3960 - JI: 0.0395 - val_loss: 0.3950 - val_JI: 0.0442\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3956 - JI: 0.0399 - val_loss: 0.3942 - val_JI: 0.0419\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3953 - JI: 0.0404 - val_loss: 0.3942 - val_JI: 0.0369\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3961 - JI: 0.0406 - val_loss: 0.3948 - val_JI: 0.0429\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3970 - JI: 0.0404 - val_loss: 0.3954 - val_JI: 0.0390\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3969 - JI: 0.0412 - val_loss: 0.3957 - val_JI: 0.0376\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3989 - JI: 0.0414 - val_loss: 0.3981 - val_JI: 0.0398\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3962 - JI: 0.0419 - val_loss: 0.3940 - val_JI: 0.0437\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3953 - JI: 0.0430 - val_loss: 0.4053 - val_JI: 0.0432\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4062 - JI: 0.0430 - val_loss: 0.3962 - val_JI: 0.0405\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3958 - JI: 0.0430 - val_loss: 0.3998 - val_JI: 0.0413\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3953 - JI: 0.0437 - val_loss: 0.3944 - val_JI: 0.0442\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3950 - JI: 0.0440 - val_loss: 0.3940 - val_JI: 0.0396\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3958 - JI: 0.0442 - val_loss: 0.3942 - val_JI: 0.0437\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3964 - JI: 0.0447 - val_loss: 0.3940 - val_JI: 0.0461\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.3946 - JI: 0.0451 - val_loss: 0.3942 - val_JI: 0.0505\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.3947 - JI: 0.0453 - val_loss: 0.3940 - val_JI: 0.0504\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.3941 - JI: 0.0462 - val_loss: 0.3960 - val_JI: 0.0415\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3949 - JI: 0.0458 - val_loss: 0.3945 - val_JI: 0.0431\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3941 - JI: 0.0465 - val_loss: 0.3993 - val_JI: 0.0508\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.3950 - JI: 0.0469 - val_loss: 0.3994 - val_JI: 0.0458\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.5243 - JI: 0.0472 - val_loss: 0.4237 - val_JI: 0.0479\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4128 - JI: 0.0473 - val_loss: 0.4051 - val_JI: 0.0468\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4003 - JI: 0.0472 - val_loss: 0.3971 - val_JI: 0.0482\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3958 - JI: 0.0478 - val_loss: 0.3945 - val_JI: 0.0482\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.3944 - JI: 0.0482 - val_loss: 0.3943 - val_JI: 0.0502\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3942 - JI: 0.0486 - val_loss: 0.3934 - val_JI: 0.0466\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3936 - JI: 0.0485 - val_loss: 0.3935 - val_JI: 0.0488\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.3935 - JI: 0.0489 - val_loss: 0.3929 - val_JI: 0.0474\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3934 - JI: 0.0488 - val_loss: 0.4007 - val_JI: 0.0492\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3949 - JI: 0.0490 - val_loss: 0.3934 - val_JI: 0.0469\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3933 - JI: 0.0495 - val_loss: 0.3940 - val_JI: 0.0492\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3931 - JI: 0.0499 - val_loss: 0.3933 - val_JI: 0.0478\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3943 - JI: 0.0494 - val_loss: 0.3930 - val_JI: 0.0544\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3934 - JI: 0.0504 - val_loss: 0.3935 - val_JI: 0.0520\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.3931 - JI: 0.0506 - val_loss: 0.4040 - val_JI: 0.0418\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.3975 - JI: 0.0502 - val_loss: 0.3927 - val_JI: 0.0480\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3934 - JI: 0.0511 - val_loss: 0.3928 - val_JI: 0.0468\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3929 - JI: 0.0502 - val_loss: 0.3932 - val_JI: 0.0533\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3959 - JI: 0.0508 - val_loss: 0.4250 - val_JI: 0.0492\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.4077 - JI: 0.0504 - val_loss: 0.3958 - val_JI: 0.0467\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.3962 - JI: 0.0509 - val_loss: 0.3953 - val_JI: 0.0459\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3934 - JI: 0.0512 - val_loss: 0.3940 - val_JI: 0.0510\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3930 - JI: 0.0514 - val_loss: 0.3933 - val_JI: 0.0498\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3928 - JI: 0.0513 - val_loss: 0.3933 - val_JI: 0.0492\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4048 - JI: 0.0517 - val_loss: 0.3927 - val_JI: 0.0534\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3934 - JI: 0.0520 - val_loss: 0.4000 - val_JI: 0.0466\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.3961 - JI: 0.0521 - val_loss: 0.3939 - val_JI: 0.0552\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.4001 - JI: 0.0524 - val_loss: 0.3936 - val_JI: 0.0480\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.4412 - JI: 0.0524 - val_loss: 0.4437 - val_JI: 0.0506\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.4168 - JI: 0.0530 - val_loss: 0.4013 - val_JI: 0.0533\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3983 - JI: 0.0525 - val_loss: 0.3960 - val_JI: 0.0552\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3952 - JI: 0.0527 - val_loss: 0.3943 - val_JI: 0.0483\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3935 - JI: 0.0522 - val_loss: 0.3935 - val_JI: 0.0528\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.3938 - JI: 0.0503 - val_loss: 0.3924 - val_JI: 0.0489\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3926 - JI: 0.0528 - val_loss: 0.3922 - val_JI: 0.0530\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3923 - JI: 0.0531 - val_loss: 0.3914 - val_JI: 0.0553\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.3923 - JI: 0.0540 - val_loss: 0.3926 - val_JI: 0.0472\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3922 - JI: 0.0537 - val_loss: 0.3923 - val_JI: 0.0548\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3921 - JI: 0.0540 - val_loss: 0.3919 - val_JI: 0.0575\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3922 - JI: 0.0545 - val_loss: 0.3923 - val_JI: 0.0538\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3951 - JI: 0.0549 - val_loss: 0.3929 - val_JI: 0.0514\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3923 - JI: 0.0547 - val_loss: 0.3916 - val_JI: 0.0538\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3924 - JI: 0.0553 - val_loss: 0.3914 - val_JI: 0.0545\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3923 - JI: 0.0562 - val_loss: 0.3921 - val_JI: 0.0500\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3923 - JI: 0.0558 - val_loss: 0.3914 - val_JI: 0.0556\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3920 - JI: 0.0562 - val_loss: 0.3915 - val_JI: 0.0590\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3924 - JI: 0.0571 - val_loss: 0.3918 - val_JI: 0.0492\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 2s 13ms/step - loss: 0.3921 - JI: 0.0569 - val_loss: 0.3924 - val_JI: 0.0562\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3923 - JI: 0.0565 - val_loss: 0.3925 - val_JI: 0.0605\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3922 - JI: 0.0577 - val_loss: 0.3909 - val_JI: 0.0582\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3918 - JI: 0.0571 - val_loss: 0.3913 - val_JI: 0.0574\n",
      "Epoch 133/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3918 - JI: 0.0576 - val_loss: 0.3922 - val_JI: 0.0605\n",
      "Epoch 134/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3919 - JI: 0.0577 - val_loss: 0.3924 - val_JI: 0.0602\n",
      "Epoch 135/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3922 - JI: 0.0577 - val_loss: 0.3918 - val_JI: 0.0584\n",
      "Epoch 136/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3927 - JI: 0.0578 - val_loss: 0.3938 - val_JI: 0.0635\n",
      "Epoch 137/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.5122 - JI: 0.0582 - val_loss: 0.4613 - val_JI: 0.0540\n",
      "Epoch 138/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4311 - JI: 0.0583 - val_loss: 0.4100 - val_JI: 0.0546\n",
      "Epoch 139/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.4035 - JI: 0.0583 - val_loss: 0.3983 - val_JI: 0.0598\n",
      "Epoch 140/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3961 - JI: 0.0583 - val_loss: 0.3944 - val_JI: 0.0619\n",
      "Epoch 141/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3946 - JI: 0.0586 - val_loss: 0.3921 - val_JI: 0.0582\n",
      "Epoch 142/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3929 - JI: 0.0593 - val_loss: 0.3963 - val_JI: 0.0522\n",
      "Epoch 143/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3928 - JI: 0.0596 - val_loss: 0.3933 - val_JI: 0.0586\n",
      "Epoch 144/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3928 - JI: 0.0586 - val_loss: 0.3910 - val_JI: 0.0587\n",
      "Epoch 145/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3928 - JI: 0.0591 - val_loss: 0.3914 - val_JI: 0.0578\n",
      "Epoch 146/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3924 - JI: 0.0588 - val_loss: 0.3913 - val_JI: 0.0590\n",
      "Epoch 147/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3917 - JI: 0.0600 - val_loss: 0.3921 - val_JI: 0.0628\n",
      "Epoch 148/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3916 - JI: 0.0600 - val_loss: 0.3920 - val_JI: 0.0574\n",
      "Epoch 149/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3931 - JI: 0.0593 - val_loss: 0.3920 - val_JI: 0.0502\n",
      "Epoch 150/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.3913 - JI: 0.0596 - val_loss: 0.3916 - val_JI: 0.0591\n",
      "Epoch 151/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.3933 - JI: 0.0607 - val_loss: 0.3943 - val_JI: 0.0561\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_deep_reg(X_train,y_train,X_test,y_test,1, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2888257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,477,420\n",
      "Trainable params: 1,477,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 3s 17ms/step - loss: 0.6069 - JI: 0.0354 - val_loss: 0.1228 - val_JI: 0.0023\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1540 - JI: 0.0334 - val_loss: 0.1125 - val_JI: 0.0050\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1312 - JI: 0.0259 - val_loss: 0.1166 - val_JI: 0.0046\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1506 - JI: 0.0262 - val_loss: 0.1130 - val_JI: 9.0406e-04\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1316 - JI: 0.0209 - val_loss: 0.1117 - val_JI: 0.0073\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1273 - JI: 0.0185 - val_loss: 0.1102 - val_JI: 0.0037\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1233 - JI: 0.0174 - val_loss: 0.1122 - val_JI: 0.0064\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1179 - JI: 0.0191 - val_loss: 0.1090 - val_JI: 0.0086\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1184 - JI: 0.0207 - val_loss: 0.1094 - val_JI: 0.0117\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1179 - JI: 0.0246 - val_loss: 0.1083 - val_JI: 0.0066\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1262 - JI: 0.0270 - val_loss: 0.1081 - val_JI: 0.0072\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1178 - JI: 0.0303 - val_loss: 0.1073 - val_JI: 0.0249\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1156 - JI: 0.0303 - val_loss: 0.1072 - val_JI: 0.0186\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1145 - JI: 0.0356 - val_loss: 0.1062 - val_JI: 0.0205\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1132 - JI: 0.0374 - val_loss: 0.1064 - val_JI: 0.0168\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1121 - JI: 0.0403 - val_loss: 0.1049 - val_JI: 0.0308\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1114 - JI: 0.0446 - val_loss: 0.1046 - val_JI: 0.0406\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1191 - JI: 0.0449 - val_loss: 0.1053 - val_JI: 0.0277\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1117 - JI: 0.0479 - val_loss: 0.1041 - val_JI: 0.0460\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1114 - JI: 0.0501 - val_loss: 0.1028 - val_JI: 0.0515\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1108 - JI: 0.0536 - val_loss: 0.1031 - val_JI: 0.0640\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1072 - JI: 0.0598 - val_loss: 0.1011 - val_JI: 0.0449\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1060 - JI: 0.0625 - val_loss: 0.1000 - val_JI: 0.0619\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1062 - JI: 0.0649 - val_loss: 0.0996 - val_JI: 0.0535\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1044 - JI: 0.0694 - val_loss: 0.0989 - val_JI: 0.0593\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1066 - JI: 0.0707 - val_loss: 0.1054 - val_JI: 0.0619\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1048 - JI: 0.0678 - val_loss: 0.0988 - val_JI: 0.0563\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 2s 18ms/step - loss: 0.1028 - JI: 0.0742 - val_loss: 0.0985 - val_JI: 0.0591\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.1047 - JI: 0.0756 - val_loss: 0.1000 - val_JI: 0.0758\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1028 - JI: 0.0760 - val_loss: 0.0976 - val_JI: 0.0575\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1013 - JI: 0.0771 - val_loss: 0.0976 - val_JI: 0.0638\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1006 - JI: 0.0817 - val_loss: 0.0971 - val_JI: 0.0784\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.1001 - JI: 0.0809 - val_loss: 0.0986 - val_JI: 0.0810\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1008 - JI: 0.0817 - val_loss: 0.0972 - val_JI: 0.0636\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0993 - JI: 0.0848 - val_loss: 0.0963 - val_JI: 0.0892\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1008 - JI: 0.0827 - val_loss: 0.0962 - val_JI: 0.0652\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0991 - JI: 0.0844 - val_loss: 0.0970 - val_JI: 0.0715\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0985 - JI: 0.0883 - val_loss: 0.0957 - val_JI: 0.0774\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0991 - JI: 0.0892 - val_loss: 0.0952 - val_JI: 0.0828\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0982 - JI: 0.0893 - val_loss: 0.0957 - val_JI: 0.0827\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0989 - JI: 0.0892 - val_loss: 0.0952 - val_JI: 0.0748\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0974 - JI: 0.0917 - val_loss: 0.0970 - val_JI: 0.0700\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.1019 - JI: 0.0875 - val_loss: 0.0951 - val_JI: 0.0811\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0998 - JI: 0.0912 - val_loss: 0.0953 - val_JI: 0.0844\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 2s 14ms/step - loss: 0.0991 - JI: 0.0902 - val_loss: 0.0960 - val_JI: 0.0825\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0987 - JI: 0.0910 - val_loss: 0.0953 - val_JI: 0.0980\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0972 - JI: 0.0924 - val_loss: 0.0968 - val_JI: 0.0642\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0970 - JI: 0.0941 - val_loss: 0.0946 - val_JI: 0.0893\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0988 - JI: 0.0845 - val_loss: 0.0955 - val_JI: 0.0971\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0985 - JI: 0.0928 - val_loss: 0.0953 - val_JI: 0.0740\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0978 - JI: 0.0890 - val_loss: 0.0953 - val_JI: 0.0990\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0971 - JI: 0.0934 - val_loss: 0.0961 - val_JI: 0.0647\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0960 - JI: 0.0937 - val_loss: 0.0975 - val_JI: 0.0616\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0959 - JI: 0.0943 - val_loss: 0.0943 - val_JI: 0.0984\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0950 - JI: 0.0985 - val_loss: 0.0939 - val_JI: 0.0982\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0975 - JI: 0.0966 - val_loss: 0.1031 - val_JI: 0.0633\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.1016 - JI: 0.0777 - val_loss: 0.0967 - val_JI: 0.0850\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0980 - JI: 0.0875 - val_loss: 0.0969 - val_JI: 0.0684\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0964 - JI: 0.0931 - val_loss: 0.0948 - val_JI: 0.0914\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0960 - JI: 0.0966 - val_loss: 0.1020 - val_JI: 0.0644\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0968 - JI: 0.0890 - val_loss: 0.0952 - val_JI: 0.0768\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0957 - JI: 0.0967 - val_loss: 0.0946 - val_JI: 0.0877\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0953 - JI: 0.0967 - val_loss: 0.0947 - val_JI: 0.0976\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0969 - JI: 0.0991 - val_loss: 0.0961 - val_JI: 0.0917\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0978 - JI: 0.0977 - val_loss: 0.0945 - val_JI: 0.0888\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0991 - JI: 0.0959 - val_loss: 0.0963 - val_JI: 0.0651\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0956 - JI: 0.0970 - val_loss: 0.0944 - val_JI: 0.0907\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0989 - JI: 0.0994 - val_loss: 0.0946 - val_JI: 0.0825\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0950 - JI: 0.0991 - val_loss: 0.0947 - val_JI: 0.0823\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0948 - JI: 0.1014 - val_loss: 0.0939 - val_JI: 0.1017\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0943 - JI: 0.1026 - val_loss: 0.0942 - val_JI: 0.0974\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0940 - JI: 0.1031 - val_loss: 0.0937 - val_JI: 0.0947s -\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0963 - JI: 0.0976 - val_loss: 0.0952 - val_JI: 0.0985\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0961 - JI: 0.0990 - val_loss: 0.0945 - val_JI: 0.0828\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0962 - JI: 0.0942 - val_loss: 0.0944 - val_JI: 0.0988\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0950 - JI: 0.0980 - val_loss: 0.0943 - val_JI: 0.0885\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0941 - JI: 0.1020 - val_loss: 0.0936 - val_JI: 0.0994\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0945 - JI: 0.1011 - val_loss: 0.0946 - val_JI: 0.1003\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0944 - JI: 0.1022 - val_loss: 0.0992 - val_JI: 0.0841\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0954 - JI: 0.0966 - val_loss: 0.0942 - val_JI: 0.0864\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0942 - JI: 0.1010 - val_loss: 0.0958 - val_JI: 0.0727\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0943 - JI: 0.1006 - val_loss: 0.0946 - val_JI: 0.0832\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0942 - JI: 0.1024 - val_loss: 0.0934 - val_JI: 0.1043\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.1058 - val_loss: 0.0949 - val_JI: 0.0867\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.1069 - val_loss: 0.0955 - val_JI: 0.0803\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0931 - JI: 0.1081 - val_loss: 0.0952 - val_JI: 0.0948\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0934 - JI: 0.1081 - val_loss: 0.0945 - val_JI: 0.1053\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0934 - JI: 0.1080 - val_loss: 0.0932 - val_JI: 0.1101\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0945 - JI: 0.1014 - val_loss: 0.0947 - val_JI: 0.1135\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0932 - JI: 0.1062 - val_loss: 0.0933 - val_JI: 0.1062\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0943 - JI: 0.1084 - val_loss: 0.0934 - val_JI: 0.1016\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0928 - JI: 0.1099 - val_loss: 0.0933 - val_JI: 0.1037\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0932 - JI: 0.1097 - val_loss: 0.0943 - val_JI: 0.0860\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.1075 - val_loss: 0.0943 - val_JI: 0.1036\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0928 - JI: 0.1105 - val_loss: 0.0957 - val_JI: 0.0866\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0926 - JI: 0.1104 - val_loss: 0.0963 - val_JI: 0.1067\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0927 - JI: 0.1104 - val_loss: 0.0941 - val_JI: 0.0891\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0921 - JI: 0.1149 - val_loss: 0.0931 - val_JI: 0.1114\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0933 - JI: 0.1110 - val_loss: 0.0943 - val_JI: 0.0967\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0928 - JI: 0.1103 - val_loss: 0.0937 - val_JI: 0.1008\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0921 - JI: 0.1138 - val_loss: 0.0947 - val_JI: 0.1024\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0929 - JI: 0.1108 - val_loss: 0.0943 - val_JI: 0.0872\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0936 - JI: 0.1055 - val_loss: 0.0948 - val_JI: 0.0970\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0933 - JI: 0.1064 - val_loss: 0.0938 - val_JI: 0.1030\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0928 - JI: 0.1104 - val_loss: 0.0938 - val_JI: 0.0939\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0926 - JI: 0.1129 - val_loss: 0.0956 - val_JI: 0.0939\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0937 - JI: 0.1147 - val_loss: 0.0955 - val_JI: 0.0853\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0951 - JI: 0.1030 - val_loss: 0.0952 - val_JI: 0.0760\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0930 - JI: 0.1094 - val_loss: 0.0937 - val_JI: 0.0943\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0931 - JI: 0.1078 - val_loss: 0.0936 - val_JI: 0.1150\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0921 - JI: 0.1153 - val_loss: 0.0941 - val_JI: 0.1086\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0917 - JI: 0.1179 - val_loss: 0.0930 - val_JI: 0.1104\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0938 - JI: 0.1115 - val_loss: 0.0943 - val_JI: 0.0994\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0937 - JI: 0.1076 - val_loss: 0.0951 - val_JI: 0.1020\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 17ms/step - loss: 0.0946 - JI: 0.1055 - val_loss: 0.0948 - val_JI: 0.0959\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0929 - JI: 0.1102 - val_loss: 0.0947 - val_JI: 0.0954\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0930 - JI: 0.1117 - val_loss: 0.0938 - val_JI: 0.1051\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0930 - JI: 0.1118 - val_loss: 0.0966 - val_JI: 0.0830\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0931 - JI: 0.1082 - val_loss: 0.0938 - val_JI: 0.1047\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0934 - JI: 0.1082 - val_loss: 0.0940 - val_JI: 0.0987\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0948 - JI: 0.1065 - val_loss: 0.0957 - val_JI: 0.0947\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0929 - JI: 0.1074 - val_loss: 0.0937 - val_JI: 0.1074\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0934 - JI: 0.1060 - val_loss: 0.0978 - val_JI: 0.0668\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 2s 16ms/step - loss: 0.0945 - JI: 0.0941 - val_loss: 0.0951 - val_JI: 0.0839\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0950 - JI: 0.0959 - val_loss: 0.0963 - val_JI: 0.0785\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.1023 - val_loss: 0.0950 - val_JI: 0.0793\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0927 - JI: 0.1081 - val_loss: 0.0942 - val_JI: 0.1039\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0927 - JI: 0.1089 - val_loss: 0.0956 - val_JI: 0.0778\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0931 - JI: 0.1071 - val_loss: 0.0944 - val_JI: 0.0924\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0924 - JI: 0.1088 - val_loss: 0.0940 - val_JI: 0.1018\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0935 - JI: 0.1071 - val_loss: 0.0957 - val_JI: 0.0826\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0933 - JI: 0.1040 - val_loss: 0.0942 - val_JI: 0.0954\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "def deep_model_dropout(feature_dim,label_dim, layer_num=1):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    print(\"create model. feature_dim ={}, label_dim ={}\".format(feature_dim, label_dim))\n",
    "    model.add(Dense(512, activation='relu', input_dim=feature_dim))\n",
    "    \n",
    "    for i in range(layer_num):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(label_dim, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[JI])\n",
    "    return model\n",
    "import matplotlib.pyplot as plt\n",
    "def train_deep_dropout(X_train,y_train,X_test,y_test, layer_num=1, callbacks_list = []):\n",
    "    feature_dim = X_train.shape[1]\n",
    "    label_dim = y_train.shape[1]\n",
    "    model = deep_model_dropout(feature_dim,label_dim, layer_num)\n",
    "    model.summary()\n",
    "    history = model.fit(X_train,y_train,batch_size=256, epochs=1000,callbacks=callbacks_list,validation_data=(X_test,y_test), verbose=1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "train_deep_dropout(X_train,y_train,X_test,y_test,1, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "307a2388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =2071, label_dim =300\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_67 (Dense)            (None, 512)               1060864   \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,002,732\n",
      "Trainable params: 2,002,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.4499 - JI: 0.0323 - val_loss: 0.1535 - val_JI: 0.0265\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 3s 21ms/step - loss: 0.1404 - JI: 0.0305 - val_loss: 0.1467 - val_JI: 0.0301\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1404 - JI: 0.0226 - val_loss: 0.1233 - val_JI: 8.0414e-05\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1377 - JI: 0.0129 - val_loss: 0.1155 - val_JI: 1.9475e-04\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1196 - JI: 0.0076 - val_loss: 0.1149 - val_JI: 4.8628e-05\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1218 - JI: 0.0067 - val_loss: 0.1141 - val_JI: 9.6575e-05\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1160 - JI: 0.0054 - val_loss: 0.1123 - val_JI: 9.6592e-05\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1154 - JI: 0.0036 - val_loss: 0.1122 - val_JI: 4.9573e-05\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1191 - JI: 0.0040 - val_loss: 0.1114 - val_JI: 4.7518e-04\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1329 - JI: 0.0033 - val_loss: 0.1113 - val_JI: 9.7968e-05\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1155 - JI: 0.0024 - val_loss: 0.1113 - val_JI: 2.4055e-04\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1177 - JI: 0.0020 - val_loss: 0.1113 - val_JI: 1.6518e-05\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 0.1143 - JI: 0.0015 - val_loss: 0.1113 - val_JI: 1.6485e-05\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1158 - JI: 0.0016 - val_loss: 0.1113 - val_JI: 4.9555e-05\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1152 - JI: 9.2541e-04 - val_loss: 0.1113 - val_JI: 3.2274e-04\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1287 - JI: 0.0013 - val_loss: 0.1120 - val_JI: 4.3186e-04\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1137 - JI: 8.5311e-04 - val_loss: 0.1113 - val_JI: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 0.1163 - JI: 7.0715e-04 - val_loss: 0.1113 - val_JI: 3.2512e-05\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1140 - JI: 6.1862e-04 - val_loss: 0.1132 - val_JI: 7.4259e-04\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1133 - JI: 7.0581e-04 - val_loss: 0.1113 - val_JI: 6.7462e-05\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1138 - JI: 4.4876e-04 - val_loss: 0.1113 - val_JI: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 3s 22ms/step - loss: 0.1141 - JI: 4.0380e-04 - val_loss: 0.1113 - val_JI: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.1134 - JI: 3.2939e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PINKPI~1\\AppData\\Local\\Temp/ipykernel_9612/2181308266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_deep_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\PINKPI~1\\AppData\\Local\\Temp/ipykernel_9612/3604190914.py\u001b[0m in \u001b[0;36mtrain_deep_dropout\u001b[1;34m(X_train, y_train, X_test, y_test, layer_num, callbacks_list)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_model_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1252\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1253\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1538\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    947\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_deep_dropout(X_train,y_train,X_test,y_test,3, callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0356400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
